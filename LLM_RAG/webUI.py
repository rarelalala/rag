import json
import time
import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

import chatmodel

st.set_page_config(page_title="LLM-RAG-WEB")
st.title("LLM-RAG-WEB")


@st.cache_resource
def init_model():
    model_path = "../models/Baichuan2-7B-Chat"
    print('model_path=', model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.generation_config = GenerationConfig.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True
    )
    return model, tokenizer


def clear_model_history():
    del st.session_state.messages
    # st.session_state.history1 = [st.session_state.history1[0]]  # ä¿ç•™åˆå§‹è®°å½•
    # placeholder.empty()


def clear_chat_history():
    del st.session_state.messages
    # st.session_state.history2 = [st.session_state.history2[0]]


def init_model_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages1" in st.session_state:
        for message in st.session_state.messages1:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages1 = []

    return st.session_state.messages1


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages2" in st.session_state:
        for message in st.session_state.messages2:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages2 = []

    return st.session_state.messages2


def main():
    # åˆå§‹åŒ–æ¨¡å‹
    model, tokenizer = init_model()

    # åˆ›å»ºä¾§è¾¹æ å¸ƒå±€
    sidebar_selection = st.sidebar.selectbox("é€‰æ‹©å¯¹è¯ç±»å‹", ("æ¨¡å‹å¯¹è¯", "æ–‡ä»¶å¯¹è¯"))

    if sidebar_selection == "æ¨¡å‹å¯¹è¯":
        messages1 = init_model_history()
        # print("history1:", st.session_state.history1)
        if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
            with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
                st.markdown(prompt)
            messages1.append({"role": "user", "content": prompt})
            print(f"[user] {prompt}", flush=True)
            with st.chat_message("assistant", avatar='ğŸ¤–'):
                placeholder = st.empty()

                # st.session_state.history1.append(["Human", prompt])
                # st.session_state.history1.append(["Assistant", None])
                # print("history1:", st.session_state.history1)
                start = time.time()

                for response in model.chat(tokenizer, messages1, stream=True):
                    placeholder.markdown(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()

                end = time.time()
                cost = end - start
                length = len(response)
                print(f"{length / cost}tokens/s")
                # print(prompt,response)
                # st.session_state.history1[-1][1] = response

            messages1.append({"role": "assistant", "content": response})
            print(json.dumps(messages1, ensure_ascii=False), flush=True)

            st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_model_history)

    elif sidebar_selection == "æ–‡ä»¶å¯¹è¯":
        messages2 = init_chat_history()
        # print("history2:", st.session_state.history2)

        if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
            with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
                st.markdown(prompt)
            sim_result = chatmodel.get_docs(prompt)
            new_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ä¸‹è¿°ä¿¡æ¯ä»¥æ¡ç†æ¸…æ™°çš„æ–¹å¼å›åº”æŸ¥è¯¢ã€‚è€ƒè™‘åˆ°ä½ çš„ä¸»è¦æœåŠ¡å¯¹è±¡æ˜¯è€å¹´ç¾¤ä½“ï¼Œè¯·ç¡®ä¿å›ç­”å†…å®¹æ˜“äºç†è§£ä¸”é€šä¿—æ˜“æ‡‚ã€‚
    å·²çŸ¥ä¿¡æ¯ï¼š  
    {sim_result} \n
    é—®é¢˜ï¼š
    {prompt}  \n"""
            # new_prompt =f"""åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

            #                 å·²çŸ¥å†…å®¹:
            #                 {sim_result}
            #                 é—®é¢˜:{prompt}"""
            messages2.append({"role": "user", "content": new_prompt})
            print(f"[user] {new_prompt}", flush=True)
            with st.chat_message("assistant", avatar='ğŸ¤–'):
                placeholder = st.empty()

                # st.session_state.history2.append(["Human", new_prompt])
                # st.session_state.history2.append(["Assistant", None])
                # print("history2:", st.session_state.history2)
                start = time.time()

                for response in model.chat(tokenizer, messages2, stream=True):
                    placeholder.markdown(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                end = time.time()

                cost = end - start
                length = len(response)
                print(f"{length / cost}tokens/s")
                # print(prompt,response[(len(prompt)+1):])
                # st.session_state.history2[-1][1] = response

            messages2.append({"role": "assistant", "content": response})
            print(json.dumps(messages2, ensure_ascii=False), flush=True)

            st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
